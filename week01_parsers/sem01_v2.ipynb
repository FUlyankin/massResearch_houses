{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наш любимый парсер\n",
    "\n",
    "Тут я накидал примерный каркас нашего будущего скачивателя ссылок. \n",
    "\n",
    "__Задание:__ Написать парсер для скачки ссылок и собрать все ссылки на недвижимость с ЦИАН.\n",
    "\n",
    "* Пытайтесь писать код в виде маленьких функций. Каждая функция делает что-то полезное. Из них собирайте основной цикл с условиями.\n",
    "* Не бойтесь экспериментировать, рассказывайте про свои эксперименты другим ребятам в чате. Пульте свой код в свою ветку. Делитесь своими лучшими функциями с другими. В мастер нельзя заливать свои функции без ревью. Делать ревью научимся, когда появится первый желающий залить свой код в мастер. Будем заниматься этим через пул-реквесты. \n",
    "\n",
    "* Если удалось скачать все ссылки, начинайте писать код для скачки их внутренностей. Снова много мелких функций для вытаскивания каждого элемента. Мой код с подготовки пары может вам помочь. Там много чего написано. Только там одна большая уродливая функция. Это не очень хорошо. \n",
    "\n",
    "\n",
    "Если у вас нет каких-то пакетов, ставьте их через `pip` либо `conda`: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хотите ставить их прямо через юпитер, дописывайте перед командой `!`. Это сигнал, что надо обратиться к консоли. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # пакет для парсинга\n",
    "import time       # пакет для работы с временем\n",
    "import pickle     # поможет сохранять данные\n",
    "\n",
    "from bs4 import BeautifulSoup   # пакет для работы с html-деревом\n",
    "from tqdm import tqdm_notebook  # пакет для прогресса в циклах \n",
    "\n",
    "from collections import defaultdict  # удобные словари смотри: https://python-scripts.com/import-collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот блок кода сформирует для нас ссылки, по которым мы в дальнейшем будем качать квартиры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "districts = [1, 325, 326, 4, 5, 6, 7, 8, 9, 10, 11, 151]\n",
    "\n",
    "rooms = ['room1=1', 'room2=1', 'room3=1', 'room4=1', \n",
    "         'room5=1', 'room6=1', 'room7=1', 'room9=1']\n",
    "\n",
    "objt = [1,2]\n",
    "\n",
    "def create_url(params):\n",
    "    main_page = 'https://www.cian.ru/cat.php?'\n",
    "    params_1 = 'engine_version=2&deal_type=sale&offer_type=flat'\n",
    "    params_2 = '&district={district}&{room}&region={region}&object_type={objt}'.format(**params)\n",
    "    return main_page + params_1 + params_2\n",
    "\n",
    "main_urls = [ ]\n",
    "\n",
    "for dis in districts:\n",
    "    for rm in rooms:\n",
    "        for ob in objt:\n",
    "            params = {\n",
    "                'region': 1,\n",
    "                'district': dis,\n",
    "                'room': rm,\n",
    "                'objt': ob}\n",
    "            \n",
    "            main_urls.append(create_url(params))\n",
    "            \n",
    "len(main_urls) # Ссылки, по которым надо пройтись"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшие вспомогательные функции. Перед тем, как использовать их, потестите их! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_capcha(soup):\n",
    "    \"\"\"\n",
    "        Проверяет вылезла ли капча\n",
    "\n",
    "        soup: bs4\n",
    "            html-страничка, прогнанная через Beautifulsoap\n",
    "    \"\"\"\n",
    "    \n",
    "    if soup.title.text == 'Captcha - база объявлений ЦИАН':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def is_empty(soup):\n",
    "    \"\"\"\n",
    "        Проверяет есть ли на странице что скачивать или она пустая с p=1\n",
    "\n",
    "        soup: bs4\n",
    "            html-страничка, прогнанная через Beautifulsoap\n",
    "    \"\"\"\n",
    "    \n",
    "    # нужно бы её написать :) \n",
    "    \n",
    "    ...\n",
    "\n",
    "# Любые другие полезные мелкие функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разные функции для скачки данных (работают в разных режимах, например) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url, p):\n",
    "    \"\"\"\n",
    "        Качает по ссылке url и номеру страницы p её содержимое, отдаёт в виде bs4\n",
    "    \"\"\"    \n",
    "    \n",
    "    url = url + '&p={}'.format(p)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    return soup \n",
    "\n",
    "\n",
    "def get_soup_retry(url, p, MAX_RETRIES=10):\n",
    "    \"\"\"\n",
    "        Качает по ссылке urlи номеру страницы p её содержимое, отдаёт в виде bs4\n",
    "        Пытается избежать Disconection и делает по MAX_RETRIES=10 попыток при проблемах\n",
    "    \"\"\"\n",
    "    \n",
    "    url = url + '&p={}'.format(p)\n",
    "    session = requests.Session()\n",
    "    \n",
    "    adapter = requests.adapters.HTTPAdapter(max_retries=MAX_RETRIES)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    resp = session.get(url)\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции, которые расфасовывают ссылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrefs(soup):\n",
    "    \"\"\"\n",
    "        Находит в soup все ссылки на дома (осторожно с классами)\n",
    "    \"\"\"\n",
    "    s = soup.find_all('div', {'class': '_93444fe79c--card--_yguQ'})\n",
    "    \n",
    "    hrefs = [item.find('a', {'class': 'c6e8ba5398--header--1fV2A'}).get(\"href\") \n",
    "                         for item in s]\n",
    "    return hrefs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.cian.ru/sale/flat/226177506/',\n",
       " 'https://www.cian.ru/sale/flat/225538051/',\n",
       " 'https://www.cian.ru/sale/flat/225446506/',\n",
       " 'https://www.cian.ru/sale/flat/225845779/',\n",
       " 'https://www.cian.ru/sale/flat/226150729/']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = main_urls[0]\n",
    "\n",
    "soup1 = get_soup(url,1)\n",
    "soup2 = get_soup_retry(url,1)\n",
    "\n",
    "print(is_capcha(soup1))\n",
    "print(is_capcha(soup2))\n",
    "\n",
    "hrefs = get_hrefs(soup1)\n",
    "hrefs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной контур для скачки ссылок. Будем класть их в словарик по каждому району. \n",
    "\n",
    "```\n",
    "{ \n",
    "    ссылка на район 1:   [лист из ссылок на дома], \n",
    "    ссылка на район 2:   [лист из ссылок на дома],\n",
    "  ....\n",
    " \n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Такая структура хранения данных поможет нам потом легко понять по каким районам ссылки плохо скачались. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_dict = defaultdict(list)  # словарик под ссылки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основной цикл идёт по ссылкам на районы\n",
    "for url in tqdm_notebook(main_urls):\n",
    "    \n",
    "    hrefs = []  # создали для текущего района пустой массив под ссылки на дома\n",
    "    \n",
    "    for p in range(1, 55):\n",
    "        \n",
    "        time.sleep(5)           # между запросами спим по 5 секунд для приличия (много!)\n",
    "        soup = get_soup(url, p)  # скачали данные \n",
    "        \n",
    "        # пока вылезает капча, отдыхаем по 5 минут \n",
    "        while is_capcha(soup):\n",
    "            print('Капча вылезла :(')\n",
    "            time.sleep(60*5)\n",
    "            soup = get_soup(url, p)  # внеочередная попытка пробится сквозь капчу \n",
    "            \n",
    "        \n",
    "        cur_hrefs = get_hrefs(soup)  # вытащили ссылки \n",
    "        \n",
    "        # проверяем всё ли скачали и пошли ли дубли\n",
    "        if p > 1:\n",
    "            if cur_hrefs[-1] == hrefs[-1]:\n",
    "                break\n",
    "                \n",
    "        hrefs.extend(cur_hrefs) # если нет дублей, обновили вектор с ссылками \n",
    "        \n",
    "    # внутренний цикл кончился, кладём для текущего района, url, кваритиры в словарик\n",
    "    hrefs_dict[url] = hrefs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мерзкий код падает и обижает нас, сохраняем промежуточный результат и качаем дальше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# осторожнее с именами файлов! случайно можно перезаписать готовый :( \n",
    "\n",
    "with open('data/data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно проверить всё ли ок сохранилось\n",
    "with open('data/data.pickle', 'rb') as f:\n",
    "    data_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
