{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наш любимый парсер\n",
    "\n",
    "Тут я накидал примерный каркас нашего будущего скачивателя ссылок. \n",
    "\n",
    "__Задание:__ Написать парсер для скачки ссылок и собрать все ссылки на недвижимость с ЦИАН.\n",
    "\n",
    "* Пытайтесь писать код в виде маленьких функций. Каждая функция делает что-то полезное. Из них собирайте основной цикл с условиями.\n",
    "* Не бойтесь экспериментировать, рассказывайте про свои эксперименты другим ребятам в чате. Пульте свой код в свою ветку. Делитесь своими лучшими функциями с другими. В мастер нельзя заливать свои функции без ревью. Делать ревью научимся, когда появится первый желающий залить свой код в мастер. Будем заниматься этим через пул-реквесты. \n",
    "\n",
    "* Если удалось скачать все ссылки, начинайте писать код для скачки их внутренностей. Снова много мелких функций для вытаскивания каждого элемента. Мой код с подготовки пары может вам помочь. Там много чего написано. Только там одна большая уродливая функция. Это не очень хорошо. \n",
    "\n",
    "\n",
    "Если у вас нет каких-то пакетов, ставьте их через `pip` либо `conda`: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хотите ставить их прямо через юпитер, дописывайте перед командой `!`. Это сигнал, что надо обратиться к консоли. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # пакет для парсинга\n",
    "import time       # пакет для работы с временем\n",
    "import pickle     # поможет сохранять данные\n",
    "\n",
    "from bs4 import BeautifulSoup   # пакет для работы с html-деревом\n",
    "from tqdm import tqdm_notebook  # пакет для прогресса в циклах \n",
    "\n",
    "from collections import defaultdict  # удобные словари смотри: https://python-scripts.com/import-collections\n",
    "\n",
    "from fake_useragent import UserAgent # библиотека для создания юзерагентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот блок кода сформирует для нас ссылки, по которым мы в дальнейшем будем качать квартиры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "districts = [1, 325, 326, 4, 5, 6, 7, 8, 9, 10, 11, 151]\n",
    "\n",
    "rooms = ['room1=1', 'room2=1', 'room3=1', 'room4=1', \n",
    "         'room5=1', 'room6=1', 'room7=1', 'room9=1']\n",
    "\n",
    "objt = [1,2]\n",
    "\n",
    "def create_url(params):\n",
    "    main_page = 'https://www.cian.ru/cat.php?'\n",
    "    params_1 = 'engine_version=2&deal_type=sale&offer_type=flat'\n",
    "    params_2 = '&district={district}&{room}&region={region}&object_type={objt}'.format(**params)\n",
    "    return main_page + params_1 + params_2\n",
    "\n",
    "main_urls = [ ]\n",
    "\n",
    "for dis in districts:\n",
    "    for rm in rooms:\n",
    "        for ob in objt:\n",
    "            params = {\n",
    "                'region': 1,\n",
    "                'district': dis,\n",
    "                'room': rm,\n",
    "                'objt': ob}\n",
    "            \n",
    "            main_urls.append(create_url(params))\n",
    "            \n",
    "len(main_urls) # Ссылки, по которым надо пройтись"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшие вспомогательные функции. Перед тем, как использовать их, потестите их! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_capcha(soup):\n",
    "    \"\"\"\n",
    "        Проверяет вылезла ли капча\n",
    "\n",
    "        soup: bs4\n",
    "            html-страничка, прогнанная через Beautifulsoap\n",
    "        return: bool\n",
    "            есть ли капча на страничке\n",
    "    \"\"\"\n",
    "     \n",
    "    if soup.title.text == 'Captcha - база объявлений ЦИАН':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def is_empty(soup):\n",
    "    \"\"\"\n",
    "        Проверяет есть ли на странице что скачивать или она пустая с p=1\n",
    "\n",
    "        soup: bs4\n",
    "            html-страничка, прогнанная через Beautifulsoap\n",
    "    \"\"\"\n",
    "    \n",
    "    x = 2 + 2 \n",
    "    x == 4\n",
    "    x == '4'\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Любые другие полезные мелкие функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разные функции для скачки данных (работают в разных режимах, например) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url, p):\n",
    "    \"\"\"\n",
    "        Качает по ссылке url и номеру страницы p её содержимое, отдаёт в виде bs4\n",
    "    \"\"\"    \n",
    "    \n",
    "    url = url + '&p={}'.format(p)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_soup_agent(url, p, agent=UserAgent().chrome):\n",
    "    \"\"\"\n",
    "        Качает по ссылке url и номеру страницы p её содержимое, отдаёт в виде bs4\n",
    "        Дополнительно на вход принимает юзер агента\n",
    "    \"\"\"    \n",
    "    \n",
    "    url = url + '&p={}'.format(p)\n",
    "    resp = requests.get(url, headers={'User-Agent': agent})\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    return soup \n",
    "\n",
    "\n",
    "def get_soup_retry(url, p, MAX_RETRIES=10):\n",
    "    \"\"\"\n",
    "        Качает по ссылке urlи номеру страницы p её содержимое, отдаёт в виде bs4\n",
    "        Пытается избежать Disconection и делает по MAX_RETRIES=10 попыток при проблемах\n",
    "    \"\"\"\n",
    "    \n",
    "    url = url + '&p={}'.format(p)\n",
    "    session = requests.Session()\n",
    "    \n",
    "    adapter = requests.adapters.HTTPAdapter(max_retries=MAX_RETRIES)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    resp = session.get(url)\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции, которые расфасовывают ссылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrefs(soup):\n",
    "    \"\"\"\n",
    "        Находит в soup все ссылки на дома (осторожно с классами)\n",
    "    \"\"\"\n",
    "    s = soup.find_all('div', {'class': '_93444fe79c--card--_yguQ'})\n",
    "    \n",
    "    hrefs = [item.find('a', {'class': 'c6e8ba5398--header--1fV2A'}).get(\"href\") \n",
    "                         for item in s]\n",
    "    return hrefs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = main_urls[0]\n",
    "\n",
    "soup1 = get_soup(url,1)\n",
    "soup2 = get_soup_retry(url,1)\n",
    "\n",
    "print(is_capcha(soup1))\n",
    "print(is_capcha(soup2))\n",
    "\n",
    "hrefs = get_hrefs(soup1)\n",
    "hrefs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной контур для скачки ссылок. Будем класть их в словарик по каждому району. \n",
    "\n",
    "```\n",
    "{ \n",
    "    ссылка на район 1:   [лист из ссылок на дома], \n",
    "    ссылка на район 2:   [лист из ссылок на дома],\n",
    "  ....\n",
    " \n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Такая структура хранения данных поможет нам потом легко понять по каким районам ссылки плохо скачались. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_dict = defaultdict(list)  # словарик под ссылки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_urls1 = [url for url in main_urls if 'district=1&' in url]\n",
    "len(main_urls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_urls1.index(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cian.ru/cat.php?engine_version=2&deal_type=sale&offer_type=flat&district=4&room2=1&region=1&object_type=1\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n",
      "Капча вылезла :(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-2088222b286d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mis_capcha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Капча вылезла :('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# сплю 5 минут\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# внеочередная попытка пробится сквозь капчу\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# основной цикл идёт по ссылкам на районы\n",
    "for url in main_urls1[:10]:\n",
    "    print(url)\n",
    "    \n",
    "    hrefs = []  # создали для текущего района пустой массив под ссылки на дома\n",
    "    \n",
    "    for p in range(8, 55):\n",
    "        \n",
    "        time.sleep(15)           # между запросами спим по 10 секунд для приличия (много!)\n",
    "        \n",
    "        try:\n",
    "            soup = get_soup(url, p)  # скачали данные \n",
    "        except ConnectionError:\n",
    "            print(\"Сервер вернул пустой ответ\")\n",
    "            time.sleep(30)\n",
    "            soup = get_soup(url, p) \n",
    "        \n",
    "        \n",
    "        # пока вылезает капча, отдыхаем по 5 минут \n",
    "        while is_capcha(soup):\n",
    "            print('Капча вылезла :(')\n",
    "            time.sleep(60*10)         # сплю 5 минут\n",
    "            soup = get_soup(url, p)  # внеочередная попытка пробится сквозь капчу \n",
    "            \n",
    "        \n",
    "        cur_hrefs = get_hrefs(soup)  # вытащили ссылки \n",
    "        \n",
    "        # проверяем всё ли скачали и пошли ли дубли\n",
    "        if p > 1:\n",
    "            if cur_hrefs[-1] == hrefs[-1]:\n",
    "                break\n",
    "                \n",
    "        hrefs.extend(cur_hrefs) # если нет дублей, обновили вектор с ссылками \n",
    "        \n",
    "    # внутренний цикл кончился, кладём для текущего района, url, кваритиры в словарик\n",
    "    print(\"Скачал {}\".format(len(hrefs)))\n",
    "    hrefs_dict[url] = hrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мерзкий код падает и обижает нас, сохраняем промежуточный результат и качаем дальше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10384, 10145)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = []\n",
    "for _,v in hrefs_dict2.items():\n",
    "    aa.extend(v)\n",
    "    \n",
    "len(aa), len(set(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# осторожнее с именами файлов! случайно можно перезаписать готовый :( \n",
    "with open('data/data_2room.pickle', 'wb') as f:\n",
    "    pickle.dump(hrefs_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно проверить всё ли ок сохранилось\n",
    "with open('data/data_2room.pickle', 'rb') as f:\n",
    "    data_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3988, 3732)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = []\n",
    "for _,v in data_new.items():\n",
    "    aa.extend(v)\n",
    "    \n",
    "len(aa), len(set(aa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
