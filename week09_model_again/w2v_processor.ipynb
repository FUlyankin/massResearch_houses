{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дебажим w2v процессор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.0\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "print(gensim.__version__)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Распаковка эмбедингов"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[тексты] Nx1  => \n",
    "[термы на документы] N x V  =>  \n",
    "[термы в виде векторов на документы] N x V x d => \n",
    "[средний вектор для каждого текста] N x d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./model.bin\"\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(MODEL_PATH, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.34272522, -0.08931037, -0.449796  , -0.48029754,  0.2616737 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector(\"путин_NOUN\")[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Распаковка модели: \n",
    "\n",
    "word2index =  {\"путин\": 0, \"котик\": 1, .... } \n",
    "\n",
    "embedding  =  [ 0.34272522, -0.08931037, -0.449796, ... \n",
    "               -0.449796  , -0.48029754,  0.2616737, ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "249318it [00:03, 64659.96it/s]\n"
     ]
    }
   ],
   "source": [
    "d = w2v_model.vector_size\n",
    "V = len(w2v_model.vocab) + 2\n",
    "\n",
    "# \"#UNK\" для неизвестных слов\n",
    "# \"#PAD\" заглушки, чтобы были одинаковые размерности у матриц\n",
    "\n",
    "# Мама мыла раму \n",
    "# Филипп закомитил код на гитхаб  \n",
    "\n",
    "# Мама мыла раму  #PAD  #PAD\n",
    "# #UNK закомитил код на #UNK\n",
    "\n",
    "word2index = {\"#UNK\": 0, \"#PAD\":1}\n",
    "\n",
    "embedding = np.zeros((V, d))\n",
    "\n",
    "for i,word in tqdm(enumerate(w2v_model.vocab.keys())):\n",
    "    word2index[word] = i + 2\n",
    "    embedding[i+2] = w2v_model.wv.get_vector(word)\n",
    "    \n",
    "assert embedding.shape == (V, d)\n",
    "assert len(word2index) == V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embedding.npy\", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"word2index.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word2index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "# Таблица преобразования частеречных тэгов Mystem в тэги UPoS:\n",
    "mystem2upos = {'A': 'ADJ', 'ADV': 'ADV', 'ADVPRO': 'ADV', 'ANUM': 'ADJ',\n",
    "             'APRO': 'DET', 'COM': 'ADJ', 'CONJ': 'SCONJ', 'INTJ': 'INTJ',\n",
    "             'NONLEX': 'X', 'NUM': 'NUM', 'PART': 'PART', 'PR': 'ADP', \n",
    "             'S': 'NOUN', 'SPRO': 'PRON', 'UNKN': 'X', 'V': 'VERB'}\n",
    "\n",
    "m = Mystem(entire_input=False)\n",
    "\n",
    "fuck_cnt = 0\n",
    "all_cnt = 0\n",
    "\n",
    "def tag_mystem(text='Текст нужно передать функции в виде строки!', mapping=None, postags=True):\n",
    "    # если частеречные тэги не нужны (например, их нет в модели), выставьте postags=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "    \n",
    "    global fuck_cnt\n",
    "    global all_cnt\n",
    "\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    \n",
    "    for w in processed:\n",
    "        all_cnt += 1\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if mapping:\n",
    "                if pos in mapping:\n",
    "                    pos = mapping[pos]  # здесь мы конвертируем тэги\n",
    "                else:\n",
    "                    pos = 'X'  # на случай, если попадется тэг, которого нет в маппинге\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        except:\n",
    "            fuck_cnt += 1\n",
    "            continue  # игнорируем все ошибки \n",
    "    if not postags:\n",
    "        tagged = [t.split('_')[0] for t in tagged]\n",
    "    return \" \".join(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'филипп_NOUN закомитила_NOUN код_NOUN на_ADP гитхаб_NOUN'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример\n",
    "texts = [\"Мама мыла раму\", \"desc Филипп закомитил коды на гитхаб\"] \n",
    "\n",
    "tag_mystem(texts[1], mapping=mystem2upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'филипп закомитила код на гитхаб'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_mystem(texts[1], mapping=mystem2upos, postags=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной цикл предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm 'cian_data_lemm_v1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch 'cian_data_lemm_v1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код считывает строчку, предобрабатывает её и записывает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('cian_data_v1.txt', \"r\")\n",
    "f2 = open('cian_data_lemm_v1.csv', \"w\")\n",
    "\n",
    "reader = csv.DictReader(f1, delimiter=\"\\t\")\n",
    "row1 = next(reader).copy()\n",
    "fieldnames = list(row1.keys()) + ['описание тэги', 'описание без тэгов']\n",
    "\n",
    "writer = csv.DictWriter(f2, fieldnames=fieldnames, delimiter=\"\\t\")\n",
    "\n",
    "f1.close()\n",
    "f1 = open('cian_data_v1.txt', \"r\")\n",
    "reader = csv.DictReader(f1, delimiter=\"\\t\")\n",
    "\n",
    "# НАДО ДОПИСАТЬ В ТАБЛИЦУ НАЗВАНИЯ СТОЛБЦОВ!\n",
    "\n",
    "cnt = 0\n",
    "for row in tqdm(reader):\n",
    "    cnt += 1\n",
    "    row['описание тэги'] = tag_mystem(row['описание'], mapping=mystem2upos)\n",
    "    row['описание без тэгов'] = tag_mystem(row['описание'], mapping=mystem2upos, postags=False)\n",
    "    writer.writerow(row)\n",
    "    # f2.write(text + \"\\n\")\n",
    "    \n",
    "#     if cnt > 20:\n",
    "#         break \n",
    "        \n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Векторайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('cian_data_lemm_v1.csv', sep=\"\\t\", header=None)\n",
    "\n",
    "y = df[37].apply(np.log).to_numpy()\n",
    "texts = df[38].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"word2index.pickle\", \"rb\") as f:\n",
    "    word2index = pickle.load(f)\n",
    "    \n",
    "embedding = np.load(\"embedding.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVecorizer:\n",
    "    \n",
    "    def __init__(self, word2index, embedding):\n",
    "        self.word2index = word2index\n",
    "        self.embedding = embedding\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "        self.nice_cnt = 0\n",
    "        self.all_cnt = 0 \n",
    "    \n",
    "    def fit(self, X):\n",
    "        for text in X:\n",
    "            tokens = text.split(\" \")\n",
    "            for tok in tokens:\n",
    "                self.vocabulary.add(tok)\n",
    "    \n",
    "    # без PADDING\n",
    "    def transform(self, X):\n",
    "        X_vect = np.zeros((len(X), self.embedding.shape[1]))\n",
    "\n",
    "        for i, text in enumerate(X):\n",
    "            vect = np.zeros(self.embedding.shape[1]) # копим вектор \n",
    "            cnt = 0\n",
    "            \n",
    "            tokens = text.split(\" \")\n",
    "            for tok in tokens:\n",
    "                self.all_cnt += 1\n",
    "                if (tok in self.vocabulary) and (tok in self.word2index):\n",
    "                    vect += self.embedding[self.word2index[tok]]\n",
    "                    cnt += 1\n",
    "                    self.nice_cnt += 1\n",
    "            X_vect[i] = vect/cnt\n",
    "        return X_vect\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit()\n",
    "        return self.transform()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorzer = W2vVecorizer(word2index, embedding)\n",
    "vectorzer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2456, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vect = vectorzer.transform(texts)\n",
    "X_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499925038444569"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorzer.nice_cnt/vectorzer.all_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Добавить в класс аргументы:__\n",
    "\n",
    "* Попробовать обучить линейную модель на w2v\n",
    "* Надо попробовать в наш класс добавить PADD, UNKN \n",
    "* Надо поробовать впихнуть tf-idf \n",
    "* Сварить ещё одни w2v, но на основе другой модели https://rusvectores.org/ru/models/\n",
    "* Обучить свой собственный w2v только на описаниях "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(min_df=2, max_df=0.8)\n",
    "\n",
    "texts_v = vect.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "tfidf_dict = dict(zip(feature_names, np.array(texts_v.mean(axis=0))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010757079971086944"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dict['абсолютно_adv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
