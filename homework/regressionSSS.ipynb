{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), (10000,), (10,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "X, y = make_regression(n_features=10, n_samples=10000)\n",
    "model_sk =  LinearRegression()\n",
    "model_sk.fit(X, y)\n",
    "w = model_sk.coef_\n",
    "\n",
    "X.shape, y.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем корпус под модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class AwesomeRegression(RegressorMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, tol=1e-6, lr=0.001, num_iter = 1000, penalty = None, alpha=1.0, l1_ratio=0.5, m = 2, metrics = 'MSE'):\n",
    "        self.w = None\n",
    "        self.tol = tol\n",
    "        self.lr = lr     #шаг\n",
    "        self.iter = 0\n",
    "        self.num_iter = num_iter\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.m = m\n",
    "        self.metrics = metrics\n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        if metrics == 'MSE':\n",
    "            return ((y - X@self.w).T)@(y - X@self.w)/y.size\n",
    "        if metrics == 'MAE':\n",
    "            return sum(abs((y - X@self.w).T)/y.size)\n",
    "\n",
    "    def grad_loss(self, X, y):\n",
    "        if self.metrics == 'MSE':\n",
    "            return -2*X.T@(y - X@self.w)\n",
    "        if self.metrics == 'MAE':\n",
    "            return (-X.T)@np.sign(y - X@w)\n",
    "    \n",
    "    def mse_loss_ridge(self, X, y):\n",
    "        return ((y - X@self.w).T@(y - X@self.w)+self.alpha*self.w.T@self.w)/y.size\n",
    "    \n",
    "    \n",
    "    def grad_loss_ridge(self, X, y):\n",
    "        return -2*(((y-X@self.w).T)@X-self.alpha*(self.w).T)\n",
    "    \n",
    "    \n",
    "    def mse_loss_lasso(self, X, y):\n",
    "        return (((y - X@self.w).T)@(y - X@self.w)+self.alpha*np.linalg.norm(self.w,1))/y.size\n",
    "    \n",
    "    \n",
    "    def grad_loss_lasso(self, X, y):\n",
    "        return -2*((y-X@self.w).T)@X + self.alpha*np.sign(self.w.T)  #subgradient\n",
    "    \n",
    "    \n",
    "    def mse_loss_elasticnet(self, X, y):\n",
    "        return ((y - X@self.w).T@(y - X@self.w)/(2*y.size)+self.alpha*  \\\n",
    "                (self.l1_ratio*np.linalg.norm(self.w,1)+0.5*(1-self.l1_ratio)*self.w.T@self.w))/y.size\\\n",
    "    \n",
    "    \n",
    "    def grad_loss_elasticnet(self, X, y):\n",
    "        return -2*X.T@(y - X@self.w)/(2*y.size)+self.alpha* \\\n",
    "                (self.l1_ratio*np.sign(self.w.T)+(1-self.l1_ratio)*self.alpha*w.T)\n",
    "    \n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.normal(size=(X.shape[1]))\n",
    "        \n",
    "        if self.penalty == None:\n",
    "            new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "\n",
    "            while np.any(abs(new_w - self.w) > self.tol) and self.iter < self.num_iter:\n",
    "                rand = np.random.randint(0,X.shape[0]-1,size=self.m)\n",
    "                self.iter += 1\n",
    "                self.w = new_w\n",
    "                new_w = self.w - self.lr*self.grad_loss(X[rand], y[rand])\n",
    "            \n",
    "        \n",
    "        if self.penalty == 'l1' or self.penalty == \"l1\":\n",
    "            \n",
    "            new_w = self.w - self.lr*self.grad_loss_lasso(X, y)\n",
    "\n",
    "            while np.any(abs(new_w - self.w) > self.tol) and self.iter < self.num_iter:\n",
    "                self.iter += 1\n",
    "                self.w = new_w\n",
    "                new_w = self.w - self.lr*self.grad_loss_lasso(X, y)\n",
    "                \n",
    "        if self.penalty == 'l2' or self.penalty == \"l2\":\n",
    "            \n",
    "            new_w = self.w - self.lr*self.grad_loss_ridge(X, y)\n",
    "\n",
    "            while np.any(abs(new_w - self.w) > self.tol) and self.iter < self.num_iter:\n",
    "                self.iter += 1\n",
    "                self.w = new_w\n",
    "                new_w = self.w - self.lr*self.grad_loss_ridge(X, y)\n",
    "        \n",
    "        if self.penalty == 'elasticnet' or self.penalty == \"elasticnet\":\n",
    "            \n",
    "            new_w = self.w - self.lr*self.grad_loss_elasticnet(X, y)\n",
    "\n",
    "            while np.any(abs(new_w - self.w) > self.tol) and self.iter < self.num_iter:\n",
    "                self.iter += 1\n",
    "                self.w = new_w\n",
    "                new_w = self.w - self.lr*self.grad_loss_elasticnet(X, y)\n",
    "            \n",
    "\n",
    "    def fit_SGD(self, X, y):  \n",
    "        self.w = np.random.normal(size=(X.shape[1]))\n",
    "        \n",
    "        new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "\n",
    "        while np.any(abs(new_w - self.w) > self.tol) and self.iter < self.num_iter:\n",
    "            \n",
    "            rand = np.random.randint(0,X.shape[0]-1)\n",
    "            self.iter += 1\n",
    "            self.w = new_w\n",
    "            new_w = self.w - self.lr*self.grad_loss(X[rand], y[rand])\n",
    "    \n",
    "    def fit_formula(X, y):\n",
    "        self.w = np.linalg.inv(X.T@X)@(X.T@y)\n",
    "     \n",
    "    def predict(self, X):\n",
    "        return X@self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.66143177e-10,  3.23145317e-11,  6.51840259e-10, -1.39460415e-09,\n",
       "        8.96463400e-10,  4.68664955e-10, -3.08390951e-11, -1.72530483e-09,\n",
       "        1.56690477e-09,  1.01260835e-09])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-2*X.T@(y - X@w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1404.85983692,    92.93882489,  1693.29671773, -3506.15657903,\n",
       "        2238.92562467,  1146.29125193,  -162.35304991, -4265.01570879,\n",
       "        3960.65689975,  2531.00219585])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-X.T)@np.sign(y - X@w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1.,  1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(y - X@w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):  \n",
    "        self.w = np.random.normal(size=(X.shape[1]))\n",
    "        \n",
    "        new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "\n",
    "        while np.any(abs(new_w - self.w) > self.tol) and self.iter < self.num_iter:\n",
    "            \n",
    "            rand = np.random.randint(0,X.shape[0]-1)\n",
    "            self.iter += 1\n",
    "            self.w = new_w\n",
    "            new_w = self.w - self.lr*self.grad_loss(X[rand], y[rand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AwesomeRegression(num_iter=100000,m=2,metrics='MAE')\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "model.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4920827 ,  0.89421528, -0.59415466,  3.28268326, -0.9823396 ,\n",
       "       -1.77006323, -0.55548609,  2.64698244, -3.38108839, -3.36257387])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.28467543, 45.92406008, 39.72233445, 88.51367472, 51.29294693,\n",
       "       29.60089231, 24.97097067, 67.17237199,  1.66021256, 25.7986071 ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sk.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sk.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (добить линейную регрессию): \n",
    "\n",
    "__а)__ Добавить в обучение параметр `num_iter`, чтобы не возникало такого, что из-за неадекватно подобранного параметра `tol` цикл работает вечно. Установить параметр по дефолту в `1000` шагов.\n",
    "\n",
    "__б)__ Добавить в модель возможность вводить l1 и l2 регуляризаторы (elstic_net), для этого придется выписать на бумажке фукнцию потерь и найти её дифференциал. Для l2 проблем быть не должно, для l1 придется быть поосторожнее с нулём. \n",
    "\n",
    "__в)__ Реализовать вместо текущего обычного градиентного спуска стохастический градиентный спуск по мини-батчам. Надо на каждой итерации отбирать $m$ случайных наблюдений и делать шаг градиентного спуска по ним. Параметр $m$ задаётся при объявлении модели.\n",
    "\n",
    "__г)__ Сейчас обучение идёт с помощью функции потерь MSE. Хочется, чтобы была возможность выбора. \n",
    "\n",
    "- Добавьте такую возможность в виде функций потерь MAE\n",
    "- Добавьте MAPE\n",
    "\n",
    "__HINT:__ подсказка как добавить MAPE прямо через MAE: https://yadi.sk/i/WpIWG_PYeQkLVQ\n",
    "\n",
    "- Добавьте функцию потерь Хубера\n",
    "\n",
    "__HINT:__ про неё можно почитать либо в [главе про функции потерь Дьяконова](https://alexanderdyakonov.files.wordpress.com/2018/10/book_08_metrics_12_blog1.pdf) либо в [первой главе Магнуса Катышева Персецкого.](https://yadi.sk/i/K8P9rL1eDK0Dqg)\n",
    "\n",
    "Склёрновская реализация такой модели [тут.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html)\n",
    "\n",
    "- Придумайте свою функцию потерь с логарифмами такую, чтобы она была робастной к выбросам. \n",
    "\n",
    "\n",
    "__д)__ Протестировать модель на сгенерированном датасете. Сравнить результаты с пакетной реализацией. Подумать каких ещё приблуд можно засунуть внутрь модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (логистическая регрессия): \n",
    "\n",
    "Нужно реализовать аналогичный класс для обучения логистической регрессии. Должен быть точно такой же интерфейс, но другая модель. \n",
    "\n",
    "- В качестве функции потерь возьмите `logloss`. Добавьте опциональные ругуляризаторы, модель учите стохастическим градиентным спуском. Все производные найдите руками с помощью матричного диффериенцирования. Сравните работу модели с пакетной реализацией и возрадуйтесь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
