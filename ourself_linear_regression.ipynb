{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 100), (1000,), (100,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = make_regression(n_features=100, n_samples=1000)\n",
    "model_sk =  LinearRegression( )\n",
    "model_sk.fit(X, y)\n",
    "w = model_sk.coef_\n",
    "\n",
    "X.shape, y.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем корпус под модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class AwesomeRegression(RegressorMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, tol=1e-6, lr=0.001):\n",
    "        self.w = None\n",
    "        self.tol = tol\n",
    "        self.lr = lr\n",
    "        self.iter = 0\n",
    "        \n",
    "    def mse_loss(self, X, y):\n",
    "        return ((y - X@self.w).T)@(y - X@self.w)/y.size\n",
    "\n",
    "    def grad_loss(self, X, y):\n",
    "        return -2*X.T@(y - X@self.w)/y.size\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.normal(size=(X.shape[1]))\n",
    "        \n",
    "        new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "        \n",
    "        while np.any(abs(new_w - self.w) > self.tol):\n",
    "            self.iter += 1\n",
    "            self.w = new_w\n",
    "            new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "\n",
    "    def fit_formula(X, y):\n",
    "        self.w = np.linalg.inv(X.T@X)@(X.T@y)\n",
    "     \n",
    "    def predict(self, X):\n",
    "        return X@self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9454"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AwesomeRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "model.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028147674322374604"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model_sk.coef_ - model.w, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Используйте scipy.special для вычисления численно неустойчивых функций\n",
    "# https://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special\n",
    "\n",
    "def lossf(w, X, y, l1, l2):\n",
    "    \"\"\"\n",
    "    Вычисление функции потерь.\n",
    "\n",
    "    :param w: numpy.array размера  (M,) dtype = np.float\n",
    "    :param X: numpy.array размера  (N, M), dtype = np.float\n",
    "    :param y: numpy.array размера  (N,), dtype = np.int\n",
    "    :param l1: float, l1 коэффициент регуляризатора \n",
    "    :param l2: float, l2 коэффициент регуляризатора \n",
    "    :return: float, value of loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    lossf = np.sum(np.log(1+np.exp(-y*np.dot(X,w)))) + l1*norm(w, ord=1) + l2* (norm(w,ord=2))**2\n",
    "    return lossf\n",
    "\n",
    "def gradf(w, X, y, l1, l2):\n",
    "    \"\"\"\n",
    "    Вычисление градиента функции потерь.\n",
    "\n",
    "    :param w: numpy.array размера  (M,), dtype = np.float\n",
    "    :param X: numpy.array размера  (N, M), dtype = np.float\n",
    "    :param y: numpy.array размера  (N,), dtype = np.int\n",
    "    :param l1: float, l1 коэффициент регуляризатора \n",
    "    :param l2: float, l2 коэффициент регуляризатора \n",
    "    :return: numpy.array размера  (M,), dtype = np.float, gradient vector d lossf / dw\n",
    "    \"\"\"\n",
    "    chisl = -y*np.exp(-y*(np.dot(X,w)))\n",
    "    znam = 1+ np.exp(-y*(np.dot(X,w)))\n",
    "    const = chisl/znam\n",
    "    gradw = np.sum((const*X.T),axis=1) + 2*l2*w + l1*np.sign(w)\n",
    "    return gradw\n",
    "\n",
    "\n",
    "class LR(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, lr=1, l1=1e-4, l2=1e-4, num_iter=1000, verbose=0):\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.w = None\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Обучение логистической регрессии.\n",
    "        Настраивает self.w коэффициенты модели.\n",
    "\n",
    "        Если self.verbose == True, то выводите значение \n",
    "        функции потерь на итерациях метода оптимизации. \n",
    "\n",
    "        :param X: numpy.array размера  (N, M), dtype = np.float\n",
    "        :param y: numpy.array размера  (N,), dtype = np.int\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        n, d = X.shape\n",
    "        self.w = np.random.normal(size=d)\n",
    "        \n",
    "        t = 0\n",
    "        err = [lossf(self.w, X, y, self.l1, self.l2)]     \n",
    "                \n",
    "        while t < self.num_iter:\n",
    "            t += 1\n",
    "            dw = gradf(self.w, X, y, self.l1, self.l2)\n",
    "            \n",
    "            #self.w -= dw * self.lr\n",
    "            while lossf(self.w-dw*self.lr, X, y, self.l1, self.l2) > \\\n",
    "                           lossf(self.w, X, y, self.l1, self.l2) - 0.1*self.lr*norm(dw,ord=2)**2:\n",
    "                self.lr = 0.95*self.lr\n",
    "            self.w -= dw * self.lr                        \n",
    "            \n",
    "            err.append(lossf(self.w, X, y, self.l1, self.l2))\n",
    "            if self.verbose == 1:\n",
    "                print(err[t])      \n",
    "        return self.w\n",
    "  \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание вероятности принадлежности объекта к классу 1.\n",
    "        Возвращает np.array размера (N,) чисел в отрезке от 0 до 1.\n",
    "\n",
    "        :param X: numpy.array размера  (N, M), dtype = np.float\n",
    "        :return: numpy.array размера  (N,), dtype = np.int\n",
    "        \"\"\"\n",
    "        # Вычислите вероятности принадлежности каждого \n",
    "        # объекта из X к положительному классу, используйте\n",
    "        # эту функцию для реализации LR.predict\n",
    "        probs =1/(1+np.exp(-np.dot(X,self.w)))\n",
    "        return probs\n",
    "     \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказание класса для объекта.\n",
    "        Возвращает np.array размера (N,) элементов 1 или -1.\n",
    "\n",
    "        :param X: numpy.array размера  (N, M), dtype = np.float\n",
    "        :return:  numpy.array размера  (N,), dtype = np.int\n",
    "        \"\"\"\n",
    "        # Вычислите предсказания для каждого объекта из X\n",
    "        predicts = self.predict_proba(X)\n",
    "        predicts[predicts >= 0.5] = 1\n",
    "        predicts[predicts < 0.5] = -1 \n",
    "        return predicts \n",
    "\n",
    "\n",
    "def test_work():\n",
    "    print \"Start test\"\n",
    "    X, y = make_classification(n_features=100, n_samples=1000)\n",
    "    y = 2 * (y - 0.5)\n",
    "\n",
    "    try:\n",
    "        clf = LR(lr=0.01, l1=1e-4, l2=1e-4, num_iter=1000, verbose=0)\n",
    "    except Exception:\n",
    "        assert False, \"Создание модели завершается с ошибкой\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        clf.fit(X, y)\n",
    "    except Exception:\n",
    "        assert False, \"Обучение модели завершается с ошибкой\"\n",
    "        return\n",
    "\n",
    "    assert isinstance(lossf(clf.w, X, y, 1e-3, 1e-3), float), \"Функция потерь должна быть скалярной и иметь тип np.float\"\n",
    "    assert gradf(clf.w, X, y, 1e-3, 1e-3).shape == (100,), \"Размерность градиента должна совпадать с числом параметров\"\n",
    "    assert gradf(clf.w, X, y, 1e-3, 1e-3).dtype == np.float, \"Вектор градиента, должен состоять из элементов типа np.float\"\n",
    "    assert clf.predict(X).shape == (1000,), \"Размер вектора предсказаний, должен совпадать с количеством объектов\"\n",
    "    assert np.min(clf.predict_proba(X)) >= 0, \"Вероятности должны быть не меньше, чем 0\"\n",
    "    assert np.max(clf.predict_proba(X)) <= 1, \"Вероятности должны быть не больше, чем 1\"\n",
    "    assert len(set(clf.predict(X))) == 2, \"Метод предсказывает больше чем 2 класса на двух классовой задаче\"\n",
    "    print \"End tests\"\n",
    "\n",
    "test_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
