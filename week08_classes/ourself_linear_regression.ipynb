{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 100), (1000,), (100,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = make_regression(n_features=100, n_samples=1000)\n",
    "model_sk =  LinearRegression( )\n",
    "model_sk.fit(X, y)\n",
    "w = model_sk.coef_\n",
    "\n",
    "X.shape, y.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем корпус под модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin, BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class AwesomeRegression(RegressorMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, tol=1e-6, lr=0.001):\n",
    "        self.w = None\n",
    "        self.tol = tol\n",
    "        self.lr = lr\n",
    "        self.iter = 0\n",
    "        \n",
    "    def mse_loss(self, X, y):\n",
    "        return ((y - X@self.w).T)@(y - X@self.w)/y.size\n",
    "\n",
    "    def grad_loss(self, X, y):\n",
    "        return -2*X.T@(y - X@self.w)/y.size\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.normal(size=(X.shape[1]))\n",
    "        \n",
    "        new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "        \n",
    "        while np.any(abs(new_w - self.w) > self.tol):\n",
    "            self.iter += 1\n",
    "            self.w = new_w\n",
    "            new_w = self.w - self.lr*self.grad_loss(X, y)\n",
    "\n",
    "    def fit_formula(X, y):\n",
    "        self.w = np.linalg.inv(X.T@X)@(X.T@y)\n",
    "     \n",
    "    def predict(self, X):\n",
    "        return X@self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9454"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AwesomeRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "model.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028147674322374604"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(model_sk.coef_ - model.w, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (добить линейную регрессию): \n",
    "\n",
    "__а)__ Добавить в обучение параметр `num_iter`, чтобы не возникало такого, что из-за неадекватно подобранного параметра `tol` цикл работает вечно. Установить параметр по дефолту в `1000` шагов.\n",
    "\n",
    "__б)__ Добавить в модель возможность вводить l1 и l2 регуляризаторы (elstic_net), для этого придется выписать на бумажке фукнцию потерь и найти её дифференциал. Для l2 проблем быть не должно, для l1 придется быть поосторожнее с нулём. \n",
    "\n",
    "__в)__ Реализовать вместо текущего обычного градиентного спуска стохастический градиентный спуск по мини-батчам. Надо на каждой итерации отбирать $m$ случайных наблюдений и делать шаг градиентного спуска по ним. Параметр $m$ задаётся при объявлении модели.\n",
    "\n",
    "__г)__ Сейчас обучение идёт с помощью функции потерь MSE. Хочется, чтобы была возможность выбора. \n",
    "\n",
    "- Добавьте такую возможность в виде функций потерь MAE\n",
    "- Добавьте MAPE\n",
    "\n",
    "__HINT:__ подсказка как добавить MAPE прямо через MAE: https://yadi.sk/i/WpIWG_PYeQkLVQ\n",
    "\n",
    "- Добавьте функцию потерь Хубера\n",
    "\n",
    "__HINT:__ про неё можно почитать либо в [главе про функции потерь Дьяконова](https://alexanderdyakonov.files.wordpress.com/2018/10/book_08_metrics_12_blog1.pdf) либо в [первой главе Магнуса Катышева Персецкого.](https://yadi.sk/i/K8P9rL1eDK0Dqg)\n",
    "\n",
    "Склёрновская реализация такой модели [тут.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html)\n",
    "\n",
    "- Придумайте свою функцию потерь с логарифмами такую, чтобы она была робастной к выбросам. \n",
    "\n",
    "\n",
    "__д)__ Протестировать модель на сгенерированном датасете. Сравнить результаты с пакетной реализацией. Подумать каких ещё приблуд можно засунуть внутрь модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 (логистическая регрессия): \n",
    "\n",
    "Нужно реализовать аналогичный класс для обучения логистической регрессии. Должен быть точно такой же интерфейс, но другая модель. \n",
    "\n",
    "- В качестве функции потерь возьмите `logloss`. Добавьте опциональные ругуляризаторы, модель учите стохастическим градиентным спуском. Все производные найдите руками с помощью матричного диффериенцирования. Сравните работу модели с пакетной реализацией и возрадуйтесь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
